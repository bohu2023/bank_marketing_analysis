{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Understanding the Data\n",
    "\n",
    "To gain a better understanding of the data, please read the information provided in the UCI link above, and examine the **Materials and Methods** section of the paper.  How many marketing campaigns does this data represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The datset collected is related to 17 campaigns that occureed between May 2008 and November 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Read in the Data\n",
    "\n",
    "Use pandas to read in the dataset `bank-additional-full.csv` and assign to a meaningful variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, recall_score, precision_score, precision_recall_curve, roc_curve\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn import set_config\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bank-additional-full.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Understanding the Features\n",
    "\n",
    "\n",
    "Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.\n",
    "\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Understanding the Task\n",
    "\n",
    "After examining the description and data, your goal now is to clearly state the *Business Objective* of the task.  State the objective below.\n",
    "\n",
    "**Business Objective**  \n",
    "The business objective is to find a model that can predict if a contact will subscribe the term deposit.The model will also help to identify the main characteristics that affect customer's decision making therefore increase success rate when select the potential buying customers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Tab analysis on column relations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['y'], df['job'], dropna=False, normalize=True, margins=True)\n",
    "pd.crosstab(df['y'], df['marital'], dropna=False, normalize=True, margins=True)\n",
    "pd.crosstab(df['y'], df['education'], dropna=False, normalize=True, margins=True)\n",
    "pd.crosstab(df['y'], df['default'], dropna=False, normalize=True, margins=True)\n",
    "pd.crosstab(df['y'], df['housing'], dropna=False, normalize=True, margins=True)\n",
    "pd.crosstab(df['y'], df['loan'], dropna=False, normalize=True,margins=True)\n",
    "pd.crosstab(df['y'], df['contact'], dropna=False, normalize=True, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Engineering Features\n",
    "\n",
    "Now that you understand your business objective, we will build a basic model to get started.  Before we can do this, we must work to encode the data.  Using just the bank information features (columns 1 - 7), prepare the features and target column for modeling with appropriate encoding and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = df.copy()\n",
    "#define column names\n",
    "categorical_cols = ['job','marital','education','default', 'housing', 'loan', 'contact','month','day_of_week']\n",
    "result_col = ['y']\n",
    "all_cols = categorical_cols + result_col\n",
    "\n",
    "#use 1-7 columns only\n",
    "base_data = base_data[all_cols]\n",
    "print(f\"shape before replace: {base_data.shape}\")\n",
    "\n",
    "#convert to string type\n",
    "#base_data[all_cols] = base_data[all_cols].astype('string')\n",
    "\n",
    "#check unique values\n",
    "for col in base_data:\n",
    "    print(base_data[col].unique())\n",
    "\n",
    "base_data = base_data.replace({'default': {'unknown':np.nan}})   \n",
    "base_data = base_data.replace({'housing': {'unknown':np.nan}}) \n",
    "base_data = base_data.replace({'loan': {'unknown':np.nan}})\n",
    "base_data = base_data.dropna()\n",
    "print(f\"shape after replace: {base_data.shape}\")\n",
    "\n",
    "#check unique values\n",
    "for col in base_data:\n",
    "    print(base_data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a column transformer\n",
    "transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), categorical_cols),\n",
    "                                     remainder = StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Train/Test Split\n",
    "\n",
    "With your data prepared, split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(base_data.drop(['y'], axis = 1), base_data['y'],\n",
    "                                                   random_state=442,\n",
    "                                                   stratify = base_data['y'])\n",
    "\n",
    "#plot the target column\n",
    "plt.hist(base_data['y'],color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Subscribed')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Subscribed Valut Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: A Baseline Model\n",
    "\n",
    "Before we build our first model, we want to establish a baseline.  What is the baseline performance that our classifier should aim to beat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = base_data['y'].value_counts(normalize = True)[0]\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**  \n",
    "Based on the above plot and the value count numbers, we can conclude that 87% of the customers didn't subscribe to the term deposit. In other words, if we build a model that only predict 'no', the model will have a score of 87%. Values of the target column is not evenly distributed. Our target should be to increase correct prediction for 'yes' value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: A Simple Model\n",
    "\n",
    "Use Logistic Regression to build a basic model on your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr_pipe = Pipeline([('transformer', transformer),\n",
    "                    ('lgr', LogisticRegression(max_iter = 1000))])\n",
    "lgr_pipe.fit(X_train, y_train)\n",
    "lgr_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Score the Model\n",
    "\n",
    "What is the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr_preds = lgr_pipe.predict(X_test)\n",
    "print(lgr_preds[:5])\n",
    "confusion_mat = confusion_matrix(y_test, lgr_preds, labels=['yes', 'no'])\n",
    "#tn, fp, fn, tp = confusion_mat.ravel()\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=['yes', 'no'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_acc = lgr_pipe.score(X_test, y_test)\n",
    "print(pipe_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**  \n",
    "With the basic logistic regression model, we got zero hit of the 'yes' label for the test data set. Now I want to compare it with KNeighborsClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe = Pipeline([('transformer', transformer),\n",
    "                    ('model', KNeighborsClassifier())])\n",
    "test_pipe.fit(X_train, y_train)\n",
    "preds = test_pipe.predict(X_test)\n",
    "print(preds[:5])\n",
    "confusion_mat = confusion_matrix(y_test, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_acc = test_pipe.score(X_test, y_test)\n",
    "print(pipe_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**  \n",
    "With KNeighborsClassifier, the score is down, but ture positive is increased, that means we can predict more 'yes' label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Model Comparisons\n",
    "\n",
    "Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models.  Using the default settings for each of the models, fit and score each.  Also, be sure to compare the fit time of each of the models.  Present your findings in a `DataFrame` similar to that below:\n",
    "\n",
    "| Model | Train Time | Train Accuracy | Test Accuracy |\n",
    "| ----- | ---------- | -------------  | -----------   |\n",
    "|     |    |.     |.     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common function to evaluate the pipeline**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(classifier, result, classifier_name):\n",
    "    \"\"\"\n",
    "    Trains a pipeline and evaluates its performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier: An instance of classifier.\n",
    "    - result: result dataframe provided to append the result to\n",
    "    - classifier_name: name of the classifier\n",
    "    \n",
    "    Returns:\n",
    "    - training_time: The time taken to train the pipeline.\n",
    "    - test_score: Accuracy of the pipeline on the test set.\n",
    "    - train_score: Accuracy of the pipeline on the training set.\n",
    "    - result: the evaluation result appended to the result dataframe\n",
    "    \"\"\"\n",
    "    # Create the pipeline\n",
    "    my_pipe = Pipeline([\n",
    "        ('transform', transformer), \n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    my_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the training time\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # Evaluate the pipeline\n",
    "    test_score = my_pipe.score(X_test, y_test)\n",
    "    train_score = my_pipe.score(X_train, y_train)\n",
    "    \n",
    "    row = pd.DataFrame([{'Model': classifier_name, 'Train Time': training_time, 'Train Accuracy':train_score, 'Test Accuracy':test_score}])\n",
    "    result = pd.concat([result, row], ignore_index=True)\n",
    "  \n",
    "    #return training_time, test_score, train_score\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run different classifiers with default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create result dataframe\n",
    "result = pd.DataFrame(columns=['Model', 'Train Time', 'Train Accuracy', 'Test Accuracy'])\n",
    "\n",
    "#run LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "result = evaluate_pipeline(classifier, result, 'Logistic Regression')\n",
    "\n",
    "#run KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()\n",
    "result = evaluate_pipeline(classifier, result, 'KNN')\n",
    "\n",
    "#run Decision Tree\n",
    "classifier = DecisionTreeClassifier(max_depth=None)\n",
    "result = evaluate_pipeline(classifier, result, 'Decision Tree')\n",
    "\n",
    "#run SVM\n",
    "classifier = SVC()\n",
    "result = evaluate_pipeline(classifier, result, 'SVC')\n",
    "\n",
    "#run dummy classifier\n",
    "classifier = DummyClassifier()\n",
    "result = evaluate_pipeline(classifier, result, 'Dummy Classifier')\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: Improving the Model\n",
    "\n",
    "Now that we have some basic models on the board, we want to try to improve these.  Below, we list a few things to explore in this pursuit.\n",
    "\n",
    "- More feature engineering and exploration.  For example, should we keep the gender feature?  Why or why not?\n",
    "- Hyperparameter tuning and grid search.  All of our models have additional hyperparameters to tune and explore.  For example the number of neighbors in KNN or the maximum depth of a Decision Tree.  \n",
    "- Adjust your performance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning for KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([('transform', transformer), ('knn', KNeighborsClassifier())])\n",
    "\n",
    "params = {'knn__n_neighbors': list(range(1, 21, 2))}\n",
    "#params = {\n",
    "#          'knn__weights': ['uniform', 'distance'],\n",
    "#          'knn__p': [1, 2]}\n",
    "\n",
    "#params = {'knn__n_neighbors': list(range(1, 22, 2))}\n",
    "grid = GridSearchCV(knn_pipe, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "#best_k = list(knn_grid.best_params_.values())[0]\n",
    "#best_acc = knn_grid.score(X_test, y_test)\n",
    "\n",
    "grid_train_acc = grid.score(X_train, y_train)\n",
    "grid_test_acc = grid.score(X_test, y_test)\n",
    "best_params = grid.best_params_\n",
    "\n",
    "print(f'Training Accuracy: {grid_train_acc: .2f}')\n",
    "print(f'Trest Accuracy: {grid_test_acc: .2f}')\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning for Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipe = Pipeline([('transform', transformer), ('model', DecisionTreeClassifier())])\n",
    "params = {'model__max_depth': [2,3, 4, 5, 8, 10],\n",
    "         'model__min_samples_split': [0.05, 0.1, 0.2, 0.05],\n",
    "          'model__criterion': [\"gini\", \"entropy\"],\n",
    "          'model__min_samples_leaf': [0.05, 0.1, 0.2, 0.05]\n",
    "         }\n",
    "\n",
    "grid = GridSearchCV(tree_pipe, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid_train_acc = grid.score(X_train, y_train)\n",
    "grid_test_acc = grid.score(X_test, y_test)\n",
    "best_params = grid.best_params_\n",
    "\n",
    "print(f'Training Accuracy: {grid_train_acc: .2f}')\n",
    "print(f'Trest Accuracy: {grid_test_acc: .2f}')\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning for SVC**  \n",
    "This tuning is very time cosuming, so I actually run with different kernal for few times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline([('transform', transformer), ('model', SVC())])\n",
    "params = {'model__kernel': ['rbf', 'poly', 'linear', 'sigmoid'],\n",
    "         'model__gamma': [0.1, 1.0, 10.0, 100.0],}\n",
    "\n",
    "grid = GridSearchCV(svc_pipe, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid_train_acc = grid.score(X_train, y_train)\n",
    "grid_test_acc = grid.score(X_test, y_test)\n",
    "best_params = grid.best_params_\n",
    "\n",
    "print(f'Training Accuracy: {grid_train_acc: .2f}')\n",
    "print(f'Trest Accuracy: {grid_test_acc: .2f}')\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run with optimized Hyperparameter** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create result dataframe\n",
    "result = pd.DataFrame(columns=['Model', 'Train Time', 'Train Accuracy', 'Test Accuracy'])\n",
    "\n",
    "#run LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "result = evaluate_pipeline(classifier, result, 'Logistic Regression')\n",
    "\n",
    "#run KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=19, weights='uniform', p=1 )\n",
    "result = evaluate_pipeline(classifier, result, 'KNN')\n",
    "\n",
    "#run Decision Tree\n",
    "classifier = DecisionTreeClassifier(criterion='gini', max_depth=2, min_samples_leaf=0.01, min_samples_split=0.01 )\n",
    "result = evaluate_pipeline(classifier, result, 'Decision Tree')\n",
    "\n",
    "#run SVM\n",
    "classifier = SVC(kernel='linear', gamma=0.1)\n",
    "result = evaluate_pipeline(classifier, result, 'SVC')\n",
    "\n",
    "#run dummy classifier\n",
    "classifier = DummyClassifier()\n",
    "result = evaluate_pipeline(classifier, result, 'Dummy Classifier')\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run Decision Tree\n",
    "result = pd.DataFrame(columns=['Model', 'Train Time', 'Train Accuracy', 'Test Accuracy'])\n",
    "#classifier = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=0.01, min_samples_split=0.01 )\n",
    "classifier = SVC(kernel='rbf', gamma=0.1)\n",
    "result = evaluate_pipeline(classifier, result, 'Decision Tree')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**  \n",
    "After cross compare of the training time and the score, the best model to choose is: **decision tree**  \n",
    "But, we would also like to check the Confusion Matrices for each model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrices for KNeighborsClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe = Pipeline([('transformer', transformer),\n",
    "                    ('model', KNeighborsClassifier(n_neighbors=19, weights='uniform', p=1))])\n",
    "test_pipe.fit(X_train, y_train)\n",
    "preds = test_pipe.predict(X_test)\n",
    "print(preds[:5])\n",
    "confusion_mat = confusion_matrix(y_test, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrices for DecisionTreeClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe = Pipeline([('transformer', transformer),\n",
    "                    ('model', DecisionTreeClassifier(criterion='gini', max_depth=6, min_samples_leaf=0.01, min_samples_split=0.01 ))])\n",
    "test_pipe.fit(X_train, y_train)\n",
    "preds = test_pipe.predict(X_test)\n",
    "print(preds[:5])\n",
    "confusion_mat = confusion_matrix(y_test, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrices for SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe = Pipeline([('transformer', transformer),\n",
    "                    ('model', SVC(kernel='linear', gamma=0.1))])\n",
    "test_pipe.fit(X_train, y_train)\n",
    "preds = test_pipe.predict(X_test)\n",
    "print(preds[:5])\n",
    "confusion_mat = confusion_matrix(y_test, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**  \n",
    "After compared the Confusion Matrices, the best model to choose is: **KNeighborsClassifier** since we are able to target the 'yes' labeled customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to add more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_2 = df.copy()\n",
    "#define column names\n",
    "categorical_cols = ['job','marital','education','default', 'housing', 'loan', 'contact','month','day_of_week','poutcome'] \n",
    "result_col = ['y']\n",
    "number_cols = ['age','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed','duration','campaign','pdays']\n",
    "all_cols = categorical_cols + result_col + number_cols\n",
    "\n",
    "base_data_2 = base_data_2[all_cols]\n",
    "#base_data_2['previous'] = base_data_2['previous'].astype(float)\n",
    "#base_data[all_cols] = base_data[all_cols].astype('string')\n",
    "\n",
    "#use all columns \n",
    "base_data_2 = base_data_2.replace({'default': {'unknown':np.nan}})   \n",
    "base_data_2 = base_data_2.replace({'housing': {'unknown':np.nan}}) \n",
    "base_data_2 = base_data_2.replace({'loan': {'unknown':np.nan}})\n",
    "base_data_2 = base_data_2.dropna()\n",
    "print(f\"shape after replace: {base_data_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a column transformer\n",
    "transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), categorical_cols),\n",
    "                                     remainder = StandardScaler())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(base_data_2.drop(['y'], axis = 1), base_data_2['y'],\n",
    "                                                   random_state=443,\n",
    "                                                   stratify = base_data_2['y'])\n",
    "\n",
    "#plot the target column\n",
    "plt.hist(base_data_2['y'],color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Subscribed')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Subscribed Valut Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipe = Pipeline([('transformer', transformer),\n",
    "                    ('model', KNeighborsClassifier(n_neighbors=19, weights='uniform', p=1))])\n",
    "test_pipe.fit(X_train, y_train)\n",
    "preds = test_pipe.predict(X_test)\n",
    "print(preds[:5])\n",
    "confusion_mat = confusion_matrix(y_test, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = test_pipe.score(X_test, y_test)\n",
    "train_score = test_pipe.score(X_train, y_train)\n",
    "print(f\"test score {test_score}\")\n",
    "print(f\"train score {train_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Based on above analysis, the best model to choose is KNeighborsClassifier with more features than just bank client data.  \n",
    "\n",
    "### Next Steps and Recommandations\n",
    "Next step, we can use numberic features to re-run the process and compare the confusion matrices. The conclusion may lead to a different model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
